# Config file used to build TorchAccelerator. All keys below are treated as
# shared variables in bash scripts by sourcing this file.


# Cuda base docker
cuda_base_docker="nvidia/cuda:12.1.1-cudnn8-devel-ubuntu20.04"

# Name and docker file for creating base docker.
base_docker="base:r2.3.0-cuda12.1.0-py3.10"
base_dockerfile="Dockerfile.base"

# Name and docker file for creating docker with TorchAccelerator.
docker_with_torchacc="acc:r2.3.0-cuda12.1.0-py3.10-nightly"
release_dockerfile="Dockerfile.release"

# Compile folder shared by pytorch and accelerator. Compiling accelerator
# depends the intermediate compile files of pytorch.
work_dir="/workspace/"

# cuda build config
cuda_compute_capabilities="7.0 8.0 8.6 9.0"

# bazel version. bazel version got from xla/third_party/tensorflow/.bazelversion
bazel_version=6.5.0

# use proxy by setting to 1. otherwise, leave it empty.
use_proxy=

torch_git=git@github.com:AlibabaPAI/pytorch.git
torch_branch=v2.3.0
xla_git=git@github.com:AlibabaPAI/xla.git
xla_branch=acc
torchacc_git=git@github.com:AlibabaPAI/torchacc.git
torchacc_branch=main

# build mode: develop or bdist_wheel
build_mode=bdist_wheel

# build cache
cache_path=~/torchacc_build_cache

# push to hub by setting to true. otherwise, false
push_to_hub=true
regions="hangzhou wulanchabu"

# do cleanup after docker building
do_clean=true
